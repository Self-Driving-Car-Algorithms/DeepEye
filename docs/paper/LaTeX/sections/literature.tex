\chapter{Related Work}
\label{ch:Literature}
%(3-5 pages)
%-------------------------------------------%


\section{Deep Convolutional Neural Networks}
%-------------------------------------------%
In 2014, \cite{3D-Traffic-Scene-Understanding-From-Movable-Platforms} introduced a novel probabilistic generative model for multi-object traffic scene understanding. Their model does not rely on GPS, LIDAR or map knowledge. Instead, it takes advantage of a well-defined set of visual cues in the form of vanishing points, semantic scene labels, scene flow, and occupancy grids. They examined their model on 113 representations of intersections, and showed that their approach can recognize urban intersections reliably with an accuracy of up to 90\%. For most of these intersections, traditional approaches based on lane markings and curbstones would have failed due to the absence of these feature cues. They ultimately proposed that utilizing Markov Chain Monte Carlo sampling to evaluate traffic scenes would improve it in terms of object detection and object orientation estimation in challenging and cluttered urban environments. In our project we will be considering a similar approach due to the lack of on-board sensory input to help us create an accurate 3D map of the scene. Instead, we will rely solely on visual cues to estimate the layout of urban intersections based on visual representation of the scene alone.
\par \bigskip 

Later on, in 2015, \cite{Google-Inception} at Google proposed Inception, a 22-layer deep convolutional neural network architecture for classification and detection. While using 12 times fewer parameters than the previous state-of-the-art architecture of \cite{AlexNet} (also known as AlexNet), the network was able to be significantly more accurate. The team adopted the network-in-network approach to amplify the representational power of neural networks. However, as noted by the development team there are two issues that come with massively DNNs architecture: the big challenge of overfitting, which is typically associated with networks that have a huge number of parameters, and the intensive use of more computational resources as the networks gets bigger and bigger. Interestingly, the team has managed to overcome both challenges by replacing the fully connected layer in the CNN with multiple, yet simpler spare layers, and spreading them across the network architecture. 
\par
The current state-of-the-art deep neural network for object detection is noted in the Regions with Convolutional Neural Networks (R-CNN) architecture by \cite{R-CNN} at UC Berkeley. The team developed a very interesting approach by breaking down the problem of object detection into two subcategories: utilizing very sophisticated region proposal algorithms to generate accurate bounding boxes, and optimizing CNN classifiers to identify object classes and categories. However, R-CNNs were initially computationally expensive due to the massive amount of resources spent on region proposals. Fortunately, the design was further improved and optimized at Microsoft by \cite{RPN}. The team proposed a Region Proposal Network (RPN) architecture, which performs both object localization through bounding boxes and classification simultaneously at each position. The model was fully optimized to run in parallel on GPUs at near real-time frame rates. Their source code is available \href{https://github.com/rbgirshick/py-faster-rcnn}{@RPN-Python}. 
\par
There are many other types of deep CNNs that may, or may not, share some of the attributes of the models noted above. However, in our project we will be looking closely at these three architectures (AlexNet, Google's Inception, and RPNs) as some of the top-5 state-of-the-art object detectors to figure out the model that would be best suited for our application in the settings of vision-based autonomous driving. \cite{Speed-accuracy-trade-offs}  
\par\bigskip
%-------------------------------------------%



\section{Deep Learning Optimization} 
%-------------------------------------------%
In the summer of 2015, \cite{Deep-Neural-Networks-are-Easily-Fooled-High-Confidence-Predictions} revealed that Deep Neural Networks (DNNs) are easily fooled to mistakenly classify an unrecognizable image as some other recognizable object with high confidence prediction by distorting the image using evolutionary algorithms. For example, it is fairly easy to artificially generate images that are totally unrecognizable to humans (e.g. a random collection of black and white distorted lines), but DNNs would mistakenly classify such images as recognizable objects (e.g. labeling distorted black and white images as a penguin) with above 90\% confidence. That leads to the bigger question in our project, which relates to the efficiency of utilizing video games for training and testing computer vision models for autonomous driving. Particularly, how much noise in an image is acceptable until a DNN trained with slightly noisy data generated from video games starts to mislabel new datasets. We will try to investigate this issue further in our project. 
\par \bigskip


Remarkably, there has been a huge debate in the industry whether computer-generated (CG) imagery has enough resemblance to real-life images in order to improve the performance of computer vision models in practice. The paper by \cite{Play-and-Learn-Using-Video-Games-to-Train-Computer-Vision-Models} presented several experiments to demonstrate that systems trained on synthetic RGB images may be used to improve the performance of CNNs on both image segmentation and depth estimation. They trained a ConvNet with over 60,000 frames that are extracted from Grand Theft Auto V (GTA-V), and showed that a CNN trained on synthetic data achieves a similar test result to a network that is trained on real-world data. They advocated that a mixed training dataset extracted from the real world and the state-of-the-art game engines gives the most improvement. We will be investigating a hybrid training strategy, by training our model on both datasets from the real world as well as images synthesized by video games and test its performance. 
\par \bigskip


Interestingly, \cite{Intel-Playing-for-Data} at Intel Labs used Grand Theft Auto V to create a large training data set for computer vision applications. They explained that deep neural networks like Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs) have been proven over and over to be the best solution in almost all computer vision problems. One of the key factors of constructing successful deep artificial neural networks (DNNs) is the abundance of massive datasets for both training and testing. However, collecting large datasets has been very expensive and time consuming. For example, each frame has to be traced and labeled, which took between 60 and 90 minutes per frame. Hence, tremendous efforts have been devoted recently to finding efficient and cost-effective ways to collect sufficiently large datasets. The team created a software layer that sits between the game and the hardware that extracts frames from the game before they are rendered. The frames are automatically labeled for all the different objects in view using a hash function. This can then be fed to a machine-learning algorithm, which will identify the objects.  The team gathered 25,000 images from the game, and labeled them all within 49 hours. Using a traditional approach would have taken at least 12 years. The team also found that models trained with $\frac{2}{3}$ game data and $\frac{1}{3}$ CamVid (dashcam footage) data outperformed models trained entirely with CamVid data alone. Their Sample code for reading the label maps and a split into training/validation/test set is available  \href{https://download.visinf.tu-darmstadt.de/data/from_games/index.html}{@IntelLabsGTAV}.
\par \bigskip


Finally, \cite{Driving-in-the-Matrix} at the University of Michigan expanded on the work from the team at Intel Labs.  They created a fully automated system that gathered and annotated training data for other vehicles, so that humans did not have to be in the loop during the annotation. They only used data gathered from synthetic environments, rather than $\frac{1}{3}$ real-world data.  They also used open-source plugins to capture the data from the GPU buffer data.  This data is analyzed to detect vehicles, and computations are made to draw tight bounding boxes around any detected vehicle without any human intervention.  As previously stated, this program only detects other vehicles, so practical usage of this would require expansion for pedestrians, objects in the road, etc.  However, this research shows that a training data set extracted solely from synthetic environments is viable for object detection. Code and dataset for reproducing the results is available \href{https://github.com/umautobots/driving-in-the-matrix}{@DrivingInTheMatrix}.
\par \bigskip
%-------------------------------------------%


\section{Deep Learning Frameworks}
%-------------------------------------------%
As discussed before, there are many different CNN architectures. Our intention was to select the best-suited architecture (particularly the right speed/accuracy balance for our base model) that would serve efficiently given our target platform and limited hardware. As expected, we ended up experimenting with many different pre-trained models. During our experiment, we came across a very interesting CNN model called YOLO Net \cite{YOLO-v1,YOLO-v2}. The model provided there would be by far the best real-time object detection system achieving great speed/accuracy balance. However, there are two problems with using it: first, the model is built off of Darknet \cite{darknet} and written in C. We would need a framework like Darkflow to act as a bridge to translate it to Tensorflow \cite{Tensorflow} and Python.  Second, Darknet is only supported by a small community as opposed to Tensorflow, which has a huge community of developers at Google and around the world. Given that our intention is to make an open source platform for ADAS, we chose to use Tensorflow, which would guarantee better support in the long run. 
\par \bigskip
%-------------------------------------------%
